run: # Configuration for a particular evaluation run
  evaluation_run_name: "human_generated_ragas_nova_micro"
  mode: "local"                   # "local" or "lambda"
  max_concurrent_async_tasks: 20
  persist_rag_outputs: true

rag_app:
  local_entrypoint: "src.rag_lambda.main:main"
  # lambda_function_name: "my-rag-lambda-prod" # uncomment this if you want to run the evaluation on a lambda function


data:
  eval_csv_path: "data/human_generated_validation_questions.csv"
  eval_id_column: "validation_question_id"
  eval_question_column: "validation_question"
  eval_reference_column: "answer"
  eval_citation_column: "citation"
  eval_source_column: "source"
  # optional extra columns auto-loaded into metadata

metrics:
  ragas:
    enabled: true
    metric_names: ["faithfulness", "answer_relevancy", "context_precision", "context_recall"]
    judge_model:
      provider: "bedrock" # E.g. "openai" or "bedrock"
      model: "amazon.nova-micro-v1:0"  
      region_name: "us-east-1"
  binary_correctness:
    enabled: true
    judge_model:
      provider: "bedrock"  # E.g. "openai" or "bedrock"
      model: "amazon.nova-micro-v1:0"  
      region_name: "us-east-1"
  atomic_correctness:
    enabled: false
    judge_model:
      provider: "bedrock"
      model: "amazon.nova-micro-v1:0"
      region_name: "us-east-1"
  context_relevance:
    enabled: false
    judge_model:
      provider: "bedrock"
      model: "amazon.nova-micro-v1:0"
      region_name: "us-east-1"

llm:
  default:
    provider: "bedrock"            # "openai" or "bedrock"
    model: "amazon.nova-micro-v1:0"
    region_name: "us-east-1"

outputs:
  types: ["html", "json", "csv"]  # CLI can override
  local:
    base_dir: "evals/eval_outputs"
  s3:
    enabled: false
    s3_uri: "s3://my-eval-bucket/rag-evals/irc/"
    # Uploads are base_dir/experiment_name/{files} -> s3_uri/experiment_name/...

