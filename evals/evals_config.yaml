run: # Configuration for a particular evaluation run
  evaluation_run_name: "example_evaluation_1"
  mode: "local"                   # "local" or "lambda"
  max_concurrent_async_tasks: 20

rag_app:
  local_entrypoint: "src.rag_lambda.main:main"
  # lambda_function_name: "my-rag-lambda-prod" # uncomment this if you want to run the evaluation on a lambda function


data:
  eval_csv_path: "data/validation_questions.csv"
  eval_id_column: "validation_question_id"
  eval_question_column: "validation_question"
  eval_reference_column: "human_reference_answer"
  eval_citation_column: "human_citation"
  # optional extra columns auto-loaded into metadata

metrics:
  ragas:
    enabled: true
    metric_names: ["faithfulness", "answer_relevancy", "context_precision", "context_recall"]
  correctness:
    enabled: true
    implementation: "binary"      # or "atomic"
    judge_model:
      provider: "openai"          # "openai" or "bedrock"
      model_name: "gpt-4o-mini"
      # provider-specific fields:
      openai:
        api_key_env: "OPENAI_API_KEY"
      bedrock:
        region_name: "us-east-1"
        model_id: "anthropic.claude-3-sonnet-20240229-v1:0"

llm:
  default:
    provider: "openai"            # "openai" or "bedrock"
    model_name: "gpt-4o-mini"
    # provider-specific fields:
    openai:
      api_key_env: "OPENAI_API_KEY"
    bedrock:
      region_name: "us-east-1"
      model_id: "anthropic.claude-3-sonnet-20240229-v1:0"

outputs:
  types: ["html", "json", "csv"]  # CLI can override
  local:
    base_dir: "eval_outputs"
  s3:
    enabled: false
    bucket: "my-eval-bucket"
    prefix: "rag-evals/irc/"
    # Uploads are base_dir/experiment_name/{files} -> s3://bucket/prefix/...

judge_validation:
  enabled: true
  csv_path: "data/judge_validation_samples.csv"
  id_column: "id"
  question_column: "question"
  reference_column: "reference_answer"
  model_answer_column: "model_answer"
  human_label_column: "human_label"

